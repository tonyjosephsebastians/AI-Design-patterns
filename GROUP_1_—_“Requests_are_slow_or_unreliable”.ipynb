{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNd8klQmXKJdwRG4huYkkqS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonyjosephsebastians/AI-Design-patterns/blob/main/GROUP_1_%E2%80%94_%E2%80%9CRequests_are_slow_or_unreliable%E2%80%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîµ GROUP 1 ‚Äî Requests Are Slow or Unreliable  \n",
        "## Foundation Patterns (Start Here)\n",
        "\n",
        "This group addresses the **most common and dangerous failure** in AI systems:\n",
        "\n",
        "> **AI workloads are slow, flaky, and expensive ‚Äî but beginners design APIs as if they are fast and reliable.**\n",
        "\n",
        "We will intentionally build a **bad system first**, feel the pain, and then fix it step by step.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Patterns Covered in Group 1\n",
        "\n",
        "We learn these **together** because they solve the *same class of problems*:\n",
        "\n",
        "1. Sync vs Async Execution  \n",
        "2. Long-Running Task Pattern  \n",
        "3. Job / Workflow Pattern  \n",
        "4. Retry + Backoff Pattern  \n",
        "5. Timeout Pattern  \n",
        "6. Circuit Breaker Pattern  \n",
        "7. Partial Result Pattern  \n",
        "8. Graceful Degradation Pattern  \n",
        "\n",
        "---\n",
        "\n",
        "# üß™ STEP 1 ‚Äî CREATE THE FAILURE  \n",
        "**(DO THIS FIRST ‚Äî DO NOT SKIP)**\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Goal of Step 1\n",
        "\n",
        "Understand **why synchronous APIs break** for AI workloads.\n",
        "\n",
        "If you don‚Äôt *feel* this failure, the patterns will feel abstract.\n",
        "\n",
        "---\n",
        "\n",
        "## üß± What We Will Build (INTENTIONALLY BAD)\n",
        "\n",
        "A naive API that:\n",
        "- uploads a document\n",
        "- does ‚Äúheavy AI processing‚Äù (OCR, embeddings, etc.)\n",
        "- **blocks the HTTP request**\n",
        "\n",
        "‚ö†Ô∏è This is exactly how many beginner AI APIs are built.\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Colab Cell 1 ‚Äî Install Dependencies\n",
        "\n",
        "```bash\n",
        "!pip install fastapi uvicorn nest_asyncio\n"
      ],
      "metadata": {
        "id": "Wk3wY1i4x84p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn nest_asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nItF7JNKw8X1",
        "outputId": "e15a1faa-f774-4724-ab05-1efe59a16024"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.123.10)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.40.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.50.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.12.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.51.0,>=0.40.0->fastapi) (4.12.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi) (3.11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import nest_asyncio\n",
        "from fastapi import FastAPI\n",
        "from fastapi.responses import JSONResponse\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/documents\")\n",
        "def uploadDocument():\n",
        "  time.sleep(20)\n",
        "  return JSONResponse({\"status\":\"done\"})"
      ],
      "metadata": {
        "id": "HCYqoUeGyM0d"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uvicorn\n",
        "import asyncio\n",
        "\n",
        "config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, loop=\"asyncio\")\n",
        "server = uvicorn.Server(config)\n",
        "asyncio.run(server.serve())"
      ],
      "metadata": {
        "id": "o5-cWkuWyzqT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "1af42c27-85d1-43cb-9f88-06704f7d5541"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [259]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [259]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-667588818.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muvicorn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"0.0.0.0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"asyncio\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mserver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muvicorn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_destroy_pending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36m_run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                     \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0;31m# restore the current task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/events.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSystemExit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;31m# instead of `__next__()`, which is slower for futures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;31m# that return non-generator iterators from their `__iter__`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Needed to break cycles when an exception occurs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0m_enter_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__step_run_and_handle_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0m_leave_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\u001b[0m in \u001b[0;36mserve\u001b[0;34m(self, sockets)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msockets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_signals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_serve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msockets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\u001b[0m in \u001b[0;36mcapture_signals\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# done LIFO, see https://stackoverflow.com/questions/48434964\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcaptured_signal\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_captured_signals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptured_signal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhandle_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFrameType\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PATTERN #1 ‚Äî Sync vs Async Execution\n",
        "\n",
        "Anything that may take more than a few seconds must be asynchronous."
      ],
      "metadata": {
        "id": "vVy5vE88zQ3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why Sync APIs Fail for AI\n",
        "\n",
        "Sync APIs assume:\n",
        "\n",
        "fast execution\n",
        "\n",
        "reliable downstream services\n",
        "\n",
        "no retries\n",
        "\n",
        "AI workloads are:\n",
        "\n",
        "slow\n",
        "\n",
        "flaky\n",
        "\n",
        "retry-prone\n",
        "\n",
        "rate-limited (429s)\n",
        "\n",
        "‚ùå Sync + AI = broken system"
      ],
      "metadata": {
        "id": "rVU8SnxmzdDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Type of Work\tAPI Style\n",
        "Chat responses\tSync (often streaming)\n",
        "OCR, embeddings, indexing\tAsync\n",
        "Agent workflows\tAsync\n",
        "Batch extraction\tAsync\n",
        "\n",
        "Async by default for AI pipelines"
      ],
      "metadata": {
        "id": "qTIaSgPJzkYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 2 ‚Äî APPLY THE FIRST FIX"
      ],
      "metadata": {
        "id": "hGSx3HeNzrvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduce an Async Boundary\n",
        "üéØ Goal of Step 2\n",
        "\n",
        "Return immediately to the client\n",
        "\n",
        "Move heavy work out of the request lifecycle\n",
        "\n",
        "This introduces two patterns at once:\n",
        "\n",
        "Sync vs Async Execution\n",
        "\n",
        "Long-Running Task Pattern"
      ],
      "metadata": {
        "id": "C5088Ts1zxsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CkKa7pCw3kf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "import time\n",
        "from fastapi import BackgroundTasks\n",
        "\n",
        "jobs = {}  # in-memory job store (temporary)\n",
        "\n",
        "@app.post(\"/documents\")\n",
        "def uploadDocument(background_tasks: BackgroundTasks):\n",
        "  jobId = str(uuid.uuid4())\n",
        "  jobs[jobId] = \"pending\"\n",
        "  background_tasks.add_task(processDocument, jobId)\n",
        "  return JSONResponse({\n",
        "        \"job_id\": job_id,\n",
        "        \"status\": \"queued\"\n",
        "    })\n",
        "\n",
        "\n",
        "def process_document(job_id: str):\n",
        "    jobs[job_id][\"status\"] = \"running\"\n",
        "    time.sleep(20)  # simulate heavy AI work\n",
        "    jobs[job_id][\"status\"] = \"done\""
      ],
      "metadata": {
        "id": "0WFis4WzzYgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.get(\"/jobs/{job_id}\")\n",
        "def get_job_status(job_id: str):\n",
        "      return jobs.get(job_id, {\"error\": \"job not found\"})\n"
      ],
      "metadata": {
        "id": "bd6S_iDw4WB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PATTERNS YOU JUST LEARNED\n",
        "‚úÖ Pattern 1 ‚Äî Sync vs Async Execution\n",
        "\n",
        "API responds immediately\n",
        "\n",
        "Work continues in background\n",
        "\n",
        "‚úÖ Pattern 2 ‚Äî Long-Running Task Pattern\n",
        "\n",
        "Heavy AI work detached from HTTP request\n",
        "\n",
        "No blocking of workers\n",
        "\n",
        "‚úÖ Pattern 3 ‚Äî Job / Workflow Pattern\n",
        "\n",
        "Explicit job identifier\n",
        "\n",
        "Client polls job status\n",
        "\n",
        "Work is trackable"
      ],
      "metadata": {
        "id": "dW_y99Ip40Pg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Current limitations:\n",
        "\n",
        "‚ùå No failure handling\n",
        "\n",
        "‚ùå No retries\n",
        "\n",
        "‚ùå No timeout protection\n",
        "\n",
        "‚ùå No circuit breaker\n",
        "\n",
        "‚ùå No progress tracking"
      ],
      "metadata": {
        "id": "vKaRyqcN49Oa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîµ GROUP 1 ‚Äî Step 3  \n",
        "## Job States + Failure Handling  \n",
        "**(Workflow Pattern + State Pattern ‚Äî GoF)**\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why Step 3 exists (Failure First)\n",
        "\n",
        "Right now, our async version stores only:\n",
        "\n",
        "```python\n",
        "jobs[job_id] = {\"status\": \"queued\"}\n"
      ],
      "metadata": {
        "id": "GYm9VF_a5kD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What breaks?\n",
        "\n",
        "If background processing crashes, the job can get stuck forever\n",
        "\n",
        "There is no progress visibility\n",
        "\n",
        "Failures are not captured\n",
        "\n",
        "No clear lifecycle guarantees\n",
        "\n",
        "So we need a real job lifecycle (state machine)."
      ],
      "metadata": {
        "id": "D0g5EvBC5opE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "e will:\n",
        "\n",
        "Define explicit job states\n",
        "\n",
        "Track progress\n",
        "\n",
        "Capture errors\n",
        "\n",
        "Enforce a simple, predictable lifecycle\n",
        "\n",
        "Patterns learned here:\n",
        "\n",
        "‚úÖ Job / Workflow Pattern\n",
        "\n",
        "‚úÖ State Pattern (GoF)\n",
        "\n",
        "‚úÖ Partial Result (foundation via progress reporting)"
      ],
      "metadata": {
        "id": "NB3xs4Pz5uJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Job State Machine\n",
        "\n",
        "Production-friendly lifecycle:\n",
        "\n",
        "queued ‚Üí running ‚Üí succeeded\n",
        "              ‚Üò failed\n",
        "\n",
        "\n",
        "(We can add cancelled later.)"
      ],
      "metadata": {
        "id": "60AYZdKB52lG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "\n",
        "class JobStatus(str, Enum):\n",
        "    QUEUED = \"queued\"\n",
        "    RUNNING = \"running\"\n",
        "    SUCCEEDED = \"succeeded\"\n",
        "    FAILED = \"failed\"\n"
      ],
      "metadata": {
        "id": "-FpzqxK-41bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "\n",
        "jobs = {}\n",
        "\n",
        "def create_job():\n",
        "    job_id = str(uuid.uuid4())\n",
        "    jobs[job_id] = {\n",
        "        \"id\": job_id,\n",
        "        \"status\": JobStatus.QUEUED,\n",
        "        \"progress\": 0,\n",
        "        \"error\": None\n",
        "    }\n",
        "    return job_id\n"
      ],
      "metadata": {
        "id": "UAtTE_GSKn7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def process_document(job_id: str):\n",
        "    try:\n",
        "        jobs[job_id][\"status\"] = JobStatus.RUNNING\n",
        "        jobs[job_id][\"progress\"] = 10\n",
        "\n",
        "        # Simulate work steps (OCR, chunking, embeddings...)\n",
        "        time.sleep(2)\n",
        "        jobs[job_id][\"progress\"] = 40\n",
        "\n",
        "        time.sleep(2)\n",
        "        jobs[job_id][\"progress\"] = 70\n",
        "\n",
        "        # Simulate a deterministic failure for learning\n",
        "        if job_id.endswith(\"7\"):\n",
        "            raise RuntimeError(\"Embedding service failed (simulated)\")\n",
        "\n",
        "        time.sleep(2)\n",
        "        jobs[job_id][\"progress\"] = 100\n",
        "        jobs[job_id][\"status\"] = JobStatus.SUCCEEDED\n",
        "\n",
        "    except Exception as e:\n",
        "        jobs[job_id][\"status\"] = JobStatus.FAILED\n",
        "        jobs[job_id][\"error\"] = str(e)\n"
      ],
      "metadata": {
        "id": "sLsT2mbGKrub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, BackgroundTasks, HTTPException\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/documents\")\n",
        "def upload_document(background_tasks: BackgroundTasks):\n",
        "    job_id = create_job()\n",
        "    background_tasks.add_task(process_document, job_id)\n",
        "    return {\"job_id\": job_id, \"status\": jobs[job_id][\"status\"]}\n",
        "\n",
        "@app.get(\"/jobs/{job_id}\")\n",
        "def get_job(job_id: str):\n",
        "    job = jobs.get(job_id)\n",
        "    if not job:\n",
        "        raise HTTPException(status_code=404, detail=\"Job not found\")\n",
        "    return job\n"
      ],
      "metadata": {
        "id": "hI2fkeinKvU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ob / Workflow Pattern\n",
        "\n",
        "Long-running operations become jobs with:\n",
        "\n",
        "job id\n",
        "\n",
        "status\n",
        "\n",
        "progress\n",
        "\n",
        "error\n",
        "\n",
        "‚úÖ State Pattern (GoF)\n",
        "\n",
        "Job behavior depends on state:\n",
        "\n",
        "queued\n",
        "\n",
        "running\n",
        "\n",
        "succeeded\n",
        "\n",
        "failed\n",
        "\n",
        "‚úÖ Partial Result (foundation)\n",
        "\n",
        "Progress reporting enables:\n",
        "\n",
        "better UX\n",
        "\n",
        "partial completion later"
      ],
      "metadata": {
        "id": "hrbMPsLjK_VQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîµ GROUP 1 ‚Äî Step 4  \n",
        "## Retry + Exponential Backoff  \n",
        "**(Retry Pattern + Backoff Strategy)**\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why Step 4 exists (Failure First)\n",
        "\n",
        "In Step 3, we added:\n",
        "- job states (`queued/running/succeeded/failed`)\n",
        "- progress updates\n",
        "- error capture\n",
        "\n",
        "But the system still fails badly in real life because AI dependencies are **flaky**:\n",
        "\n",
        "- LLM APIs return **429 Too Many Requests**\n",
        "- network hiccups happen\n",
        "- embedding services fail temporarily\n",
        "- search indexes sometimes time out\n",
        "\n",
        "### ‚ùå What breaks without retries?\n",
        "- One transient failure marks the job as `FAILED` permanently  \n",
        "- Users re-run jobs manually (waste money + time)  \n",
        "- System looks unreliable\n",
        "\n",
        "So we need:\n",
        "‚úÖ automatic retry  \n",
        "‚úÖ exponential backoff  \n",
        "‚úÖ a max retry limit  \n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Goal of Step 4\n",
        "\n",
        "We will:\n",
        "1. Simulate a **flaky external service**\n",
        "2. Add a **retry wrapper**\n",
        "3. Use **exponential backoff**\n",
        "4. Update the job status properly when retries fail\n",
        "\n",
        "Patterns learned here:\n",
        "- ‚úÖ Retry Pattern  \n",
        "- ‚úÖ Exponential Backoff Strategy  \n",
        "\n",
        "---\n",
        "\n",
        "This function fails randomly to mimic:\n",
        "- 429 throttling\n",
        "- network instability\n",
        "\n",
        "```python\n",
        "\n"
      ],
      "metadata": {
        "id": "uMeTRsghL9_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class RateLimitError(Exception):\n",
        "    pass\n",
        "\n",
        "def flaky_embedding_call():\n",
        "    r = random.random()\n",
        "\n",
        "    # ~35% chance of \"429\"\n",
        "    if r < 0.35:\n",
        "        raise RateLimitError(\"429 Too Many Requests (simulated)\")\n",
        "\n",
        "    # ~10% chance of other transient failure\n",
        "    if r < 0.45:\n",
        "        raise RuntimeError(\"Temporary network failure (simulated)\")\n",
        "\n",
        "    return \"ok\""
      ],
      "metadata": {
        "id": "1IB_261iLBXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def retry_with_backoff(fn, *, max_retries=5, base_delay=0.5, max_delay=8.0):\n",
        "    \"\"\"\n",
        "    Retry Pattern + Exponential Backoff\n",
        "    - base_delay grows exponentially: base_delay * 2^attempt\n",
        "    - max_delay caps the sleep\n",
        "    \"\"\"\n",
        "    attempt = 0\n",
        "    last_err = None\n",
        "\n",
        "    while attempt <= max_retries:\n",
        "        try:\n",
        "            return fn()\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            if attempt == max_retries:\n",
        "                break\n",
        "\n",
        "            delay = min(max_delay, base_delay * (2 ** attempt))\n",
        "            time.sleep(delay)\n",
        "            attempt += 1\n",
        "\n",
        "    raise last_err\n"
      ],
      "metadata": {
        "id": "QRShE5FrMGJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_document(job_id: str):\n",
        "    try:\n",
        "        jobs[job_id][\"status\"] = JobStatus.RUNNING\n",
        "        jobs[job_id][\"progress\"] = 10\n",
        "        jobs[job_id][\"error\"] = None\n",
        "\n",
        "        # Step A: OCR/chunking simulation\n",
        "        time.sleep(1)\n",
        "        jobs[job_id][\"progress\"] = 35\n",
        "\n",
        "        # Step B: Embeddings (flaky) with retries\n",
        "        # Store how many retries we used for observability\n",
        "        retries_used = 0\n",
        "\n",
        "        def call_with_count():\n",
        "            nonlocal retries_used\n",
        "            retries_used += 1\n",
        "            return flaky_embedding_call()\n",
        "\n",
        "        result = retry_with_backoff(call_with_count, max_retries=4, base_delay=0.5, max_delay=4.0)\n",
        "\n",
        "        jobs[job_id][\"progress\"] = 75\n",
        "        jobs[job_id][\"retries_used\"] = retries_used - 1  # first call isn't a \"retry\"\n",
        "\n",
        "        # Step C: Indexing simulation\n",
        "        time.sleep(1)\n",
        "        jobs[job_id][\"progress\"] = 100\n",
        "        jobs[job_id][\"status\"] = JobStatus.SUCCEEDED\n",
        "\n",
        "    except Exception as e:\n",
        "        jobs[job_id][\"status\"] = JobStatus.FAILED\n",
        "        jobs[job_id][\"error\"] = str(e)\n"
      ],
      "metadata": {
        "id": "SmuFmKZBMLjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patterns Learned\n",
        "‚úÖ Retry Pattern\n",
        "\n",
        "transient failures should be retried automatically\n",
        "\n",
        "improves reliability without user intervention\n",
        "\n",
        "‚úÖ Exponential Backoff Strategy\n",
        "\n",
        "spacing out retries reduces load\n",
        "\n",
        "avoids hammering services during outages/throttling"
      ],
      "metadata": {
        "id": "Mc1xP6yAMWya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîµ GROUP 1 ‚Äî Step 5  \n",
        "## Timeouts + Circuit Breaker  \n",
        "**(Timeout Pattern + Circuit Breaker Pattern)**\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why Step 5 exists (Failure First)\n",
        "\n",
        "In Step 4, we added:\n",
        "- retries\n",
        "- exponential backoff\n",
        "\n",
        "But retries alone are not enough.\n",
        "\n",
        "### ‚ùå What breaks without timeouts?\n",
        "- A dependency can **hang forever** (LLM call stuck, search stuck)\n",
        "- Your job stays `RUNNING` forever\n",
        "- Workers get stuck and stop processing other jobs\n",
        "\n",
        "### ‚ùå What breaks without a circuit breaker?\n",
        "- If a dependency is **down** (or constantly 429/500), retries keep hammering it\n",
        "- You waste time and money\n",
        "- You cause cascading failures across your system\n",
        "\n",
        "So we need:\n",
        "‚úÖ **Timeouts**: ‚Äúdon‚Äôt wait forever‚Äù  \n",
        "‚úÖ **Circuit breaker**: ‚Äústop calling a broken dependency temporarily‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Goal of Step 5\n",
        "\n",
        "We will:\n",
        "1. Add a **timeout wrapper** around external calls\n",
        "2. Add a **circuit breaker** that:\n",
        "   - opens after too many failures\n",
        "   - blocks calls for a cooldown period\n",
        "   - attempts recovery (half-open)\n",
        "\n",
        "Patterns learned:\n",
        "- ‚úÖ Timeout Pattern\n",
        "- ‚úÖ Circuit Breaker Pattern\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "jfpAd2uvN4lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "\n",
        "class TimeoutError(Exception):\n",
        "    pass\n",
        "\n",
        "_executor = concurrent.futures.ThreadPoolExecutor(max_workers=8)\n",
        "\n",
        "def run_with_timeout(fn, timeout_seconds: float):\n",
        "    \"\"\"\n",
        "    Timeout Pattern:\n",
        "    - run fn in a thread\n",
        "    - if it doesn't finish in timeout_seconds => raise TimeoutError\n",
        "    \"\"\"\n",
        "    future = _executor.submit(fn)\n",
        "    try:\n",
        "        return future.result(timeout=timeout_seconds)\n",
        "    except concurrent.futures.TimeoutError:\n",
        "        raise TimeoutError(f\"Timed out after {timeout_seconds}s\")"
      ],
      "metadata": {
        "id": "40IEBA73MSIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Part B ‚Äî Circuit Breaker Pattern"
      ],
      "metadata": {
        "id": "XmcKLy3BOEtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from dataclasses import dataclass\n",
        "\n",
        "class CircuitOpenError(Exception):\n",
        "    pass\n",
        "\n",
        "@dataclass\n",
        "class CircuitBreaker:\n",
        "    failure_threshold: int = 3      # how many failures before opening\n",
        "    recovery_seconds: int = 10      # how long to stay open\n",
        "    half_open_trial: int = 1        # allow 1 trial call in half-open\n",
        "\n",
        "    _failures: int = 0\n",
        "    _state: str = \"CLOSED\"          # CLOSED, OPEN, HALF_OPEN\n",
        "    _opened_at: float = 0.0\n",
        "    _half_open_used: int = 0\n",
        "\n",
        "    def allow_call(self) -> None:\n",
        "        now = time.time()\n",
        "\n",
        "        if self._state == \"OPEN\":\n",
        "            if now - self._opened_at >= self.recovery_seconds:\n",
        "                self._state = \"HALF_OPEN\"\n",
        "                self._half_open_used = 0\n",
        "            else:\n",
        "                raise CircuitOpenError(\"Circuit is OPEN ‚Äî calls blocked temporarily\")\n",
        "\n",
        "        if self._state == \"HALF_OPEN\":\n",
        "            if self._half_open_used >= self.half_open_trial:\n",
        "                raise CircuitOpenError(\"Circuit HALF_OPEN ‚Äî trial already used\")\n",
        "            self._half_open_used += 1\n",
        "\n",
        "    def record_success(self) -> None:\n",
        "        self._failures = 0\n",
        "        self._state = \"CLOSED\"\n",
        "\n",
        "    def record_failure(self) -> None:\n",
        "        self._failures += 1\n",
        "        if self._failures >= self.failure_threshold:\n",
        "            self._state = \"OPEN\"\n",
        "            self._opened_at = time.time()\n"
      ],
      "metadata": {
        "id": "DiVZuTGdOBaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "breaker = CircuitBreaker(failure_threshold=3, recovery_seconds=10, half_open_trial=1)\n",
        "\n",
        "def guarded_external_call(fn, *, timeout_seconds=3.0):\n",
        "    \"\"\"\n",
        "    Combines:\n",
        "    - Circuit Breaker Pattern (stop calling when dependency is failing)\n",
        "    - Timeout Pattern (avoid hanging forever)\n",
        "    \"\"\"\n",
        "    breaker.allow_call()\n",
        "    try:\n",
        "        result = run_with_timeout(fn, timeout_seconds=timeout_seconds)\n",
        "        breaker.record_success()\n",
        "        return result\n",
        "    except Exception:\n",
        "        breaker.record_failure()\n",
        "        raise\n"
      ],
      "metadata": {
        "id": "0PzQNK4hOM8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def flaky_or_hanging_embedding_call():\n",
        "    r = random.random()\n",
        "\n",
        "    # 20% chance to hang longer than timeout\n",
        "    if r < 0.20:\n",
        "        time.sleep(10)  # will exceed timeout_seconds\n",
        "        return \"ok\"\n",
        "\n",
        "    # 30% chance of throttling\n",
        "    if r < 0.50:\n",
        "        raise RateLimitError(\"429 Too Many Requests (simulated)\")\n",
        "\n",
        "    # 10% chance of other transient failure\n",
        "    if r < 0.60:\n",
        "        raise RuntimeError(\"Temporary network failure (simulated)\")\n",
        "\n",
        "    return \"ok\"\n"
      ],
      "metadata": {
        "id": "XptpI7TtOTD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update your job processor to use timeout + circuit breaker"
      ],
      "metadata": {
        "id": "SqJANk56OdLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_document(job_id: str):\n",
        "    try:\n",
        "        jobs[job_id][\"status\"] = JobStatus.RUNNING\n",
        "        jobs[job_id][\"progress\"] = 10\n",
        "        jobs[job_id][\"error\"] = None\n",
        "\n",
        "        # Step A: chunking simulation\n",
        "        time.sleep(1)\n",
        "        jobs[job_id][\"progress\"] = 35\n",
        "\n",
        "        # Step B: embeddings with:\n",
        "        # - circuit breaker\n",
        "        # - timeout\n",
        "        # - retry + backoff (from Step 4)\n",
        "        retries_used = 0\n",
        "\n",
        "        def call_embedding():\n",
        "            return guarded_external_call(\n",
        "                flaky_or_hanging_embedding_call,\n",
        "                timeout_seconds=2.0\n",
        "            )\n",
        "\n",
        "        def call_with_count():\n",
        "            nonlocal retries_used\n",
        "            retries_used += 1\n",
        "            return call_embedding()\n",
        "\n",
        "        # bounded retries (still important!)\n",
        "        retry_with_backoff(call_with_count, max_retries=3, base_delay=0.5, max_delay=4.0)\n",
        "\n",
        "        jobs[job_id][\"retries_used\"] = max(0, retries_used - 1)\n",
        "        jobs[job_id][\"progress\"] = 75\n",
        "\n",
        "        # Step C: indexing simulation\n",
        "        time.sleep(1)\n",
        "        jobs[job_id][\"progress\"] = 100\n",
        "        jobs[job_id][\"status\"] = JobStatus.SUCCEEDED\n",
        "\n",
        "    except CircuitOpenError as e:\n",
        "        jobs[job_id][\"status\"] = JobStatus.FAILED\n",
        "        jobs[job_id][\"error\"] = f\"CIRCUIT_OPEN: {str(e)}\"\n",
        "\n",
        "    except TimeoutError as e:\n",
        "        jobs[job_id][\"status\"] = JobStatus.FAILED\n",
        "        jobs[job_id][\"error\"] = f\"TIMEOUT: {str(e)}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        jobs[job_id][\"status\"] = JobStatus.FAILED\n",
        "        jobs[job_id][\"error\"] = str(e)\n"
      ],
      "metadata": {
        "id": "Vj1A-dkvOWYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patterns Learned\n",
        "‚úÖ Timeout Pattern\n",
        "\n",
        "Prevents ‚Äúhang forever‚Äù behavior\n",
        "\n",
        "Keeps workers healthy\n",
        "\n",
        "Forces fast failure\n",
        "\n",
        "‚úÖ Circuit Breaker Pattern\n",
        "\n",
        "Stops hammering a broken dependency\n",
        "\n",
        "Protects your system during outages\n",
        "\n",
        "Enables controlled recovery"
      ],
      "metadata": {
        "id": "vQiG0cY0OmOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîµ GROUP 1 ‚Äî Step 6  \n",
        "## Partial Results + Graceful Degradation  \n",
        "**(Partial Result Pattern + Graceful Degradation Pattern)**\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why Step 6 exists (Failure First)\n",
        "\n",
        "In Step 5 we added:\n",
        "- retries + backoff\n",
        "- **timeouts**\n",
        "- circuit breaker\n",
        "\n",
        "Now the system is *safer*, but still frustrating:\n",
        "\n",
        "### ‚ùå What breaks without partial results?\n",
        "If a job fails at 80%, the user gets **nothing**, even though:\n",
        "- OCR might be done\n",
        "- chunking might be done\n",
        "- some embeddings might be done\n",
        "- indexing might be partially done\n",
        "\n",
        "That wastes time and cost.\n",
        "\n",
        "### ‚ùå What breaks without graceful degradation?\n",
        "When dependencies fail (LLM down, embedding slow), the system should still:\n",
        "- return a simpler result\n",
        "- reduce features\n",
        "- fall back to cached or keyword-only search\n",
        "- keep UX usable\n",
        "\n",
        "So we need:\n",
        "‚úÖ store partial artifacts as we go  \n",
        "‚úÖ return something useful even when the ‚Äúbest path‚Äù fails  \n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Goal of Step 6\n",
        "\n",
        "We will:\n",
        "1. Track **stages** of the job (OCR ‚Üí chunk ‚Üí embed ‚Üí index)\n",
        "2. Store **partial outputs** in the job record\n",
        "3. If a stage fails, **degrade gracefully** instead of total failure\n",
        "4. Make the job result explain *what it did and didn‚Äôt do*\n",
        "\n",
        "Patterns learned:\n",
        "- ‚úÖ Partial Result Pattern  \n",
        "- ‚úÖ Graceful Degradation Pattern  "
      ],
      "metadata": {
        "id": "7LEGwv83Qqq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîµ GROUP 1 ‚Äî Step 6  \n",
        "## Partial Results + Graceful Degradation  \n",
        "**(Partial Result Pattern + Graceful Degradation Pattern)**\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why Step 6 exists (Failure First)\n",
        "\n",
        "In Step 5 we added:\n",
        "- retries + backoff\n",
        "- **timeouts**\n",
        "- circuit breaker\n",
        "\n",
        "Now the system is *safer*, but still frustrating:\n",
        "\n",
        "### ‚ùå What breaks without partial results?\n",
        "If a job fails at 80%, the user gets **nothing**, even though:\n",
        "- OCR might be done\n",
        "- chunking might be done\n",
        "- some embeddings might be done\n",
        "- indexing might be partially done\n",
        "\n",
        "That wastes time and cost.\n",
        "\n",
        "### ‚ùå What breaks without graceful degradation?\n",
        "When dependencies fail (LLM down, embedding slow), the system should still:\n",
        "- return a simpler result\n",
        "- reduce features\n",
        "- fall back to cached or keyword-only search\n",
        "- keep UX usable\n",
        "\n",
        "So we need:\n",
        "‚úÖ store partial artifacts as we go  \n",
        "‚úÖ return something useful even when the ‚Äúbest path‚Äù fails  \n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Goal of Step 6\n",
        "\n",
        "We will:\n",
        "1. Track **stages** of the job (OCR ‚Üí chunk ‚Üí embed ‚Üí index)\n",
        "2. Store **partial outputs** in the job record\n",
        "3. If a stage fails, **degrade gracefully** instead of total failure\n",
        "4. Make the job result explain *what it did and didn‚Äôt do*\n",
        "\n",
        "Patterns learned:\n",
        "- ‚úÖ Partial Result Pattern  \n",
        "- ‚úÖ Graceful Degradation Pattern  "
      ],
      "metadata": {
        "id": "ae2Pt7Y6R3Al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "\n",
        "class PipelineStage(str, Enum):\n",
        "    OCR = \"ocr\"\n",
        "    CHUNK = \"chunk\"\n",
        "    EMBED = \"embed\"\n",
        "    INDEX = \"index\""
      ],
      "metadata": {
        "id": "CEmAeAxHOmCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_job():\n",
        "    job_id = str(uuid.uuid4())\n",
        "    jobs[job_id] = {\n",
        "        \"id\": job_id,\n",
        "        \"status\": JobStatus.QUEUED,\n",
        "        \"progress\": 0,\n",
        "        \"error\": None,\n",
        "        \"completed_stages\": [],\n",
        "        \"warnings\": [],\n",
        "        \"artifacts\": {\n",
        "            \"text_preview\": None,\n",
        "            \"chunks_preview\": [],\n",
        "            \"embeddings_done\": 0,\n",
        "            \"indexed\": False,\n",
        "        },\n",
        "        \"result\": None\n",
        "    }\n",
        "    return job_id"
      ],
      "metadata": {
        "id": "UWIgnjeYSm9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6C ‚Äî Stage functions (simulate partial outputs)"
      ],
      "metadata": {
        "id": "pSQsObWTS47q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do_ocr():\n",
        "    # pretend we extracted text from a scanned PDF\n",
        "    time.sleep(1)\n",
        "    return \"This is extracted text from the PDF. It contains clauses and definitions...\"\n",
        "\n",
        "def do_chunking(text: str):\n",
        "    # pretend we chunked the text\n",
        "    time.sleep(1)\n",
        "    chunks = [text[i:i+40] for i in range(0, min(len(text), 160), 40)]\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "yohVlgOSS25b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_chunk(chunk: str):\n",
        "    # use guarded call from Step 5 (timeout + circuit breaker)\n",
        "    return guarded_external_call(flaky_or_hanging_embedding_call, timeout_seconds=2.0)\n",
        "\n",
        "def index_chunks(chunks):\n",
        "    # indexing simulation\n",
        "    time.sleep(1)\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "xG5-ZWF_TAdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6D ‚Äî Graceful degradation rules\n",
        "\n",
        "When embedding fails, we will:\n",
        "\n",
        "still mark OCR + chunk done\n",
        "\n",
        "set a warning\n",
        "\n",
        "degrade to ‚Äúkeyword-only mode‚Äù (simulated)\n",
        "\n",
        "mark job as succeeded with degraded mode (or ‚Äúsucceeded_with_warnings‚Äù)\n",
        "\n",
        "To keep it simple, we‚Äôll keep status SUCCEEDED but include warnings.\n",
        "(Production systems often use SUCCEEDED_WITH_WARNINGS.)"
      ],
      "metadata": {
        "id": "tH-H65fBTVM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_document(job_id: str):\n",
        "    try:\n",
        "        jobs[job_id][\"status\"] = JobStatus.RUNNING\n",
        "        jobs[job_id][\"progress\"] = 5\n",
        "\n",
        "        # Stage 1: OCR\n",
        "        text = do_ocr()\n",
        "        jobs[job_id][\"completed_stages\"].append(PipelineStage.OCR)\n",
        "        jobs[job_id][\"artifacts\"][\"text_preview\"] = text[:120]\n",
        "        jobs[job_id][\"progress\"] = 25\n",
        "\n",
        "        # Stage 2: Chunking\n",
        "        chunks = do_chunking(text)\n",
        "        jobs[job_id][\"completed_stages\"].append(PipelineStage.CHUNK)\n",
        "        jobs[job_id][\"artifacts\"][\"chunks_preview\"] = chunks[:3]\n",
        "        jobs[job_id][\"progress\"] = 50\n",
        "\n",
        "        # Stage 3: Embeddings (best effort)\n",
        "        embeddings_done = 0\n",
        "        for c in chunks:\n",
        "            try:\n",
        "                # Retry (Step 4) + Timeout/Circuit (Step 5) are applied here\n",
        "                retry_with_backoff(lambda: embed_chunk(c), max_retries=2, base_delay=0.5, max_delay=2.0)\n",
        "                embeddings_done += 1\n",
        "            except Exception as e:\n",
        "                # Graceful degradation: stop embedding, continue with what we have\n",
        "                jobs[job_id][\"warnings\"].append(\n",
        "                    f\"Degraded: embeddings stopped early due to error: {type(e).__name__}\"\n",
        "                )\n",
        "                break\n",
        "\n",
        "        jobs[job_id][\"artifacts\"][\"embeddings_done\"] = embeddings_done\n",
        "        if embeddings_done > 0:\n",
        "            jobs[job_id][\"completed_stages\"].append(PipelineStage.EMBED)\n",
        "        jobs[job_id][\"progress\"] = 75\n",
        "\n",
        "        # Stage 4: Indexing (only if embeddings done, otherwise degrade)\n",
        "        if embeddings_done > 0:\n",
        "            ok = index_chunks(chunks)\n",
        "            jobs[job_id][\"artifacts\"][\"indexed\"] = bool(ok)\n",
        "            jobs[job_id][\"completed_stages\"].append(PipelineStage.INDEX)\n",
        "        else:\n",
        "            # Degrade: keyword-only / basic text search mode\n",
        "            jobs[job_id][\"warnings\"].append(\n",
        "                \"Degraded: index skipped; system will use keyword-only search for this document.\"\n",
        "            )\n",
        "\n",
        "        jobs[job_id][\"progress\"] = 100\n",
        "        jobs[job_id][\"status\"] = JobStatus.SUCCEEDED\n",
        "\n",
        "        # Final result summary (what the system achieved)\n",
        "        jobs[job_id][\"result\"] = {\n",
        "            \"mode\": \"full_rag\" if jobs[job_id][\"artifacts\"][\"indexed\"] else \"keyword_only\",\n",
        "            \"completed_stages\": [s.value for s in jobs[job_id][\"completed_stages\"]],\n",
        "            \"warnings\": jobs[job_id][\"warnings\"],\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        # If something truly fatal happens very early\n",
        "        jobs[job_id][\"status\"] = JobStatus.FAILED\n",
        "        jobs[job_id][\"error\"] = str(e)\n"
      ],
      "metadata": {
        "id": "BZuCEhxJTDWm"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What to Observe When Testing\n",
        "\n",
        "Create jobs using POST /documents\n",
        "\n",
        "Poll GET /jobs/{job_id}\n",
        "\n",
        "You should see outcomes like:\n",
        "\n",
        "‚úÖ Success (full path)\n",
        "\n",
        "completed_stages: ocr, chunk, embed, index\n",
        "\n",
        "mode: full_rag\n",
        "\n",
        "warnings: []\n",
        "\n",
        "‚úÖ Success with degradation\n",
        "\n",
        "completed_stages: ocr, chunk (maybe embed partially)\n",
        "\n",
        "mode: keyword_only\n",
        "\n",
        "warnings includes why it degraded\n",
        "\n",
        "‚ùå Failure (rare)\n",
        "\n",
        "only if OCR/chunking fails in our simple demo\n",
        "\n",
        "üß† Patterns Learned\n",
        "‚úÖ Partial Result Pattern\n",
        "\n",
        "Store useful intermediate outputs:\n",
        "\n",
        "extracted text preview\n",
        "\n",
        "chunk previews\n",
        "\n",
        "number of embeddings completed\n",
        "\n",
        "Job still provides value even if later stages fail\n",
        "\n",
        "‚úÖ Graceful Degradation Pattern\n",
        "\n",
        "When ‚Äúbest path‚Äù fails, system switches to a simpler mode:\n",
        "\n",
        "skip indexing\n",
        "\n",
        "fall back to keyword-only search\n",
        "\n",
        "User still gets usable output + clear warnings"
      ],
      "metadata": {
        "id": "RLiLazGVT5S0"
      }
    }
  ]
}