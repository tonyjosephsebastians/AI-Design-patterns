{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCZsmlSsSPgw6dl92LYwGr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonyjosephsebastians/AI-Design-patterns/blob/main/GROUP_3_%E2%80%94_State_Breaks_When_Services_Restart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GROUP 3 ‚Äî State Breaks When Services Restart"
      ],
      "metadata": {
        "id": "qrXMwqDdan1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your system must survive restarts, never lose truth, and support replay / reprocessing."
      ],
      "metadata": {
        "id": "5YTWwG27at_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This group is üî• very common in AI pipelines, job systems, RAG, agent workflows, payments."
      ],
      "metadata": {
        "id": "b_pmU1VYbMW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Core Failure (Understand This First)\n",
        "\n",
        "\n",
        "‚ùå What goes wrong\n",
        "- App restarts (deploy, crash, scale-down)\n",
        "- In-memory state is wiped\n",
        "- Jobs disappear\n",
        "- Partial work is lost\n",
        "- System no longer knows \"what really happened\"\n",
        "\n",
        "\n",
        "Classic bad example\n",
        "# BAD: in-memory job store\n",
        "jobs = {}\n",
        "\n",
        "def submit_job(job_id, payload):\n",
        "    jobs[job_id] = \"RUNNING\"\n",
        "\n",
        "\n",
        "üí• Restart happens ‚Üí jobs = {} ‚Üí truth is gone"
      ],
      "metadata": {
        "id": "SkA5Bn3dbTuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stateless Service Pattern (FOUNDATION)"
      ],
      "metadata": {
        "id": "VXSCEb7AbkAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "üß† Idea\n",
        "\n",
        "Services should not own state.\n",
        "They compute, not remember.\n",
        "\n",
        "‚úÖ Rule\n",
        "\n",
        "No business truth in memory\n",
        "\n",
        "All state lives outside (DB, cache, log)"
      ],
      "metadata": {
        "id": "L8onyUnvaqBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before vs After"
      ],
      "metadata": {
        "id": "wfbKJbgRbtza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Service = logic + state"
      ],
      "metadata": {
        "id": "Csr7aVOKb6bC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Service = logic only\n",
        "State = DB / Log / Queue"
      ],
      "metadata": {
        "id": "vPrtAvoyb9Uk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stateless services allow horizontal scaling and safe restarts because all system truth is externalized"
      ],
      "metadata": {
        "id": "0lQ2Ap6pcCF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Externalized State Pattern\n",
        "üß† Idea\n",
        "\n",
        "Persist every meaningful state transition."
      ],
      "metadata": {
        "id": "OWZjQ7AJb-XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What counts as state?\n",
        "\n",
        "Job status\n",
        "\n",
        "Agent progress\n",
        "\n",
        "RAG ingestion status\n",
        "\n",
        "User workflow step"
      ],
      "metadata": {
        "id": "C9_iHj2pcYat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example: Job table"
      ],
      "metadata": {
        "id": "M64arBDEcbRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stateless Service Pattern ‚Üí No job truth stored in memory; all functions read/write DB.\n",
        "\n",
        "Externalized State Pattern ‚Üí SQLite tables store job + step state.\n",
        "\n",
        "Entity Decomposition Pattern ‚Üí jobs, job_steps, job_events, job_snapshots.\n",
        "\n",
        "Canonical Data Model Pattern ‚Üí Canonical job/step state transitions via events.\n",
        "\n",
        "Versioned Data Pattern ‚Üí schema_version stored in jobs + events.\n",
        "\n",
        "Append-Only / Audit Log Pattern ‚Üí job_events is INSERT-only.\n",
        "\n",
        "Replay / Reprocessing Pattern ‚Üí /jobs/{id}/replay rebuilds from events (+ snapshot).\n",
        "\n",
        "Memento Pattern ‚Üí /snapshot stores reconstructed state checkpoint.\n",
        "\n",
        "Soft Delete Pattern ‚Üí deleted_at instead of hard delete.\n",
        "\n",
        "updated_at ‚Üí job and steps update their timestamps whenever state changes (important in interviews)."
      ],
      "metadata": {
        "id": "rwJbRtGnepmk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Group 3 ‚Äî State survives restarts (FastAPI mini-project)\n",
        "\n",
        "## Run\n",
        "pip install -r requirements.txt\n",
        "uvicorn app.main:app --reload\n",
        "\n",
        "## Try\n",
        "1) Create job\n",
        "POST http://127.0.0.1:8000/jobs\n",
        "\n",
        "2) Start job\n",
        "POST /jobs/{job_id}/start\n",
        "\n",
        "3) Complete steps\n",
        "POST /jobs/{job_id}/steps/{step_name}/complete\n",
        "\n",
        "4) Snapshot\n",
        "POST /jobs/{job_id}/snapshot\n",
        "\n",
        "5) Replay to rebuild state\n",
        "POST /jobs/{job_id}/replay\n",
        "\n",
        "6) Soft delete\n",
        "DELETE /jobs/{job_id}\n",
        "\n",
        "## What this shows\n",
        "- Stateless Service: no in-memory truth\n",
        "- Externalized State: SQLite persistence\n",
        "- Entity Decomposition: jobs, steps, events, snapshots\n",
        "- Canonical + Versioned model: schema_version in event payload\n",
        "- Append-only audit log: events table only INSERT\n",
        "- Replay: rebuild job state from events\n",
        "- Memento: snapshots for faster restore\n",
        "- Soft delete: deleted_at instead of hard delete\n",
        "- updated_at: always updated when job/step changes\n"
      ],
      "metadata": {
        "id": "8ilyLXaKew8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn pydantic nest-asyncio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZdu5jxXcRvc",
        "outputId": "be2702bb-9668-4ff0-f889-683fb9fb76dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.123.10)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.40.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.50.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.51.0,>=0.40.0->fastapi) (4.12.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi) (3.11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2 ‚Äî Imports & DB Setup (Externalized State)"
      ],
      "metadata": {
        "id": "B6CpmhmDfKO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import json\n",
        "import uuid\n",
        "from datetime import datetime, timezone\n",
        "from typing import Dict, Any, Optional\n"
      ],
      "metadata": {
        "id": "L_NOz6KufHYS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DB_PATH = \"group3_state.db\"\n",
        "\n",
        "def utcnow():\n",
        "    return datetime.now(timezone.utc).isoformat()\n"
      ],
      "metadata": {
        "id": "tB2sCQnzfM66"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_db():\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    conn.execute(\"PRAGMA journal_mode=WAL;\")\n",
        "\n",
        "    conn.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS jobs(\n",
        "        job_id TEXT PRIMARY KEY,\n",
        "        status TEXT,\n",
        "        deleted_at TEXT,\n",
        "        created_at TEXT,\n",
        "        updated_at TEXT\n",
        "    )\n",
        "    \"\"\")\n",
        "\n",
        "    conn.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS job_steps(\n",
        "        step_id TEXT PRIMARY KEY,\n",
        "        job_id TEXT,\n",
        "        step_name TEXT,\n",
        "        status TEXT\n",
        "    )\n",
        "    \"\"\")\n",
        "\n",
        "    # üî• Append-only audit log\n",
        "    conn.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS job_events(\n",
        "        event_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        job_id TEXT,\n",
        "        event_type TEXT,\n",
        "        payload TEXT,\n",
        "        created_at TEXT\n",
        "    )\n",
        "    \"\"\")\n",
        "\n",
        "    # üî• Memento snapshot table\n",
        "    conn.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS job_snapshots(\n",
        "        snapshot_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        job_id TEXT,\n",
        "        snapshot TEXT,\n",
        "        created_at TEXT\n",
        "    )\n",
        "    \"\"\")\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "init_db()\n",
        "print(\"DB initialized\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWnFiJmXfPLr",
        "outputId": "75b91df6-103b-4078-e70f-85a77892d1f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DB initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def append_event(job_id: str, event_type: str, payload: Dict[str, Any]):\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    conn.execute(\n",
        "        \"\"\"\n",
        "        INSERT INTO job_events(job_id, event_type, payload, created_at)\n",
        "        VALUES (?, ?, ?, ?)\n",
        "        \"\"\",\n",
        "        (job_id, event_type, json.dumps(payload), utcnow())\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n"
      ],
      "metadata": {
        "id": "a0q2Pvj0fVIA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_STEPS = [\"INGEST\", \"EXTRACT\", \"EMBED\", \"INDEX\"]\n",
        "\n",
        "def create_job():\n",
        "    job_id = str(uuid.uuid4())\n",
        "    now = utcnow()\n",
        "\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    conn.execute(\n",
        "        \"INSERT INTO jobs VALUES (?, ?, NULL, ?, ?)\",\n",
        "        (job_id, \"CREATED\", now, now)\n",
        "    )\n",
        "\n",
        "    for step in DEFAULT_STEPS:\n",
        "        conn.execute(\n",
        "            \"INSERT INTO job_steps VALUES (?, ?, ?, ?)\",\n",
        "            (str(uuid.uuid4()), job_id, step, \"PENDING\")\n",
        "        )\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    append_event(job_id, \"JOB_CREATED\", {})\n",
        "    return job_id\n"
      ],
      "metadata": {
        "id": "-yTSNRFnfh7a"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_id = create_job()\n",
        "job_id\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dOdKNHDEfmON",
        "outputId": "8bb71ecc-349d-4f50-be35-818d464119b9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1075a6ae-6d05-49d0-9207-fbb02559748c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5 ‚Äî Update Job & Steps (Externalized State)"
      ],
      "metadata": {
        "id": "m7TlfxInfsQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def start_job(job_id: str):\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    conn.execute(\n",
        "        \"UPDATE jobs SET status=?, updated_at=? WHERE job_id=?\",\n",
        "        (\"RUNNING\", utcnow(), job_id)\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    append_event(job_id, \"JOB_STARTED\", {})\n"
      ],
      "metadata": {
        "id": "2Id_5-ZCfomy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_step(job_id: str, step_name: str):\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    conn.execute(\n",
        "        \"\"\"\n",
        "        UPDATE job_steps\n",
        "        SET status=?\n",
        "        WHERE job_id=? AND step_name=?\n",
        "        \"\"\",\n",
        "        (\"DONE\", job_id, step_name)\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    append_event(job_id, \"STEP_COMPLETED\", {\"step\": step_name})\n"
      ],
      "metadata": {
        "id": "bomk_QNaf8Kl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_step(job_id: str, step_name: str):\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    conn.execute(\n",
        "        \"\"\"\n",
        "        UPDATE job_steps\n",
        "        SET status=?\n",
        "        WHERE job_id=? AND step_name=?\n",
        "        \"\"\",\n",
        "        (\"DONE\", job_id, step_name)\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    append_event(job_id, \"STEP_COMPLETED\", {\"step\": step_name})\n"
      ],
      "metadata": {
        "id": "Za0KOymcgCD8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_job(job_id)\n",
        "complete_step(job_id, \"INGEST\")\n",
        "complete_step(job_id, \"EXTRACT\")"
      ],
      "metadata": {
        "id": "Z2k7SNuogfxU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© Cell 6 ‚Äî üî• Replay Engine (Reprocessing Pattern)"
      ],
      "metadata": {
        "id": "KH9Vy8s-gjwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replay_job(job_id: str):\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    rows = conn.execute(\n",
        "        \"SELECT event_type, payload FROM job_events WHERE job_id=? ORDER BY event_id\",\n",
        "        (job_id,)\n",
        "    ).fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    state = {\"status\": None, \"steps\": {}}\n",
        "\n",
        "    for event_type, payload in rows:\n",
        "        payload = json.loads(payload)\n",
        "\n",
        "        if event_type == \"JOB_CREATED\":\n",
        "            state[\"status\"] = \"CREATED\"\n",
        "        elif event_type == \"JOB_STARTED\":\n",
        "            state[\"status\"] = \"RUNNING\"\n",
        "        elif event_type == \"STEP_COMPLETED\":\n",
        "            state[\"steps\"][payload[\"step\"]] = \"DONE\"\n",
        "\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "7tSq8g2HgiGQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replay_job(job_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zpoP6YxgpJT",
        "outputId": "1732b963-571b-4305-c7e5-04e53175cd8a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'RUNNING', 'steps': {'INGEST': 'DONE', 'EXTRACT': 'DONE'}}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 7 ‚Äî üî• Snapshot (Memento Pattern)"
      ],
      "metadata": {
        "id": "IpBlMJydg_L6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_snapshot(job_id: str):\n",
        "    state = replay_job(job_id)\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    conn.execute(\n",
        "        \"\"\"\n",
        "        INSERT INTO job_snapshots(job_id, snapshot, created_at)\n",
        "        VALUES (?, ?, ?)\n",
        "        \"\"\",\n",
        "        (job_id, json.dumps(state), utcnow())\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n"
      ],
      "metadata": {
        "id": "Q7cb6eUEgrC9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 8 ‚Äî Restart Simulation"
      ],
      "metadata": {
        "id": "L6K8NYIZhGgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After restart, DB still exists\n",
        "replay_job(job_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-skON4thCrD",
        "outputId": "f1d7df5a-ce6b-41c5-a585-e815bedbda1a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'RUNNING', 'steps': {'INGEST': 'DONE', 'EXTRACT': 'DONE'}}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No in-memory variables\n",
        "\n",
        "State rebuilt from DB\n",
        "\n",
        "This is exactly what interviewers want"
      ],
      "metadata": {
        "id": "KlE_Q_vkhL5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def soft_delete(job_id: str):\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    conn.execute(\n",
        "        \"UPDATE jobs SET deleted_at=?, updated_at=? WHERE job_id=?\",\n",
        "        (utcnow(), utcnow(), job_id)\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    append_event(job_id, \"JOB_SOFT_DELETED\", {})\n"
      ],
      "metadata": {
        "id": "3cEGTbmNhJPn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soft_delete(job_id)\n",
        "replay_job(job_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auXhKXtohQWr",
        "outputId": "647496f6-5bbc-4d2b-e80f-84511c9b8b54"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'RUNNING', 'steps': {'INGEST': 'DONE', 'EXTRACT': 'DONE'}}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Big Idea (Group 3) in One Sentence\n",
        "\n",
        "Your app must never store ‚Äútruth‚Äù only in memory, because memory disappears on restart.\n",
        "\n",
        "So we design the system so that:\n",
        "\n",
        "memory can be empty any time,\n",
        "\n",
        "and the system can still rebuild the truth from storage.\n",
        "\n",
        "A Simple Story (Best way to remember)\n",
        "\n",
        "Imagine you are running a restaurant kitchen.\n",
        "\n",
        "‚ùå Bad system (in-memory truth)\n",
        "\n",
        "You write all orders on a whiteboard.\n",
        "\n",
        "Customer order comes ‚Üí you write on whiteboard\n",
        "\n",
        "Kitchen is cooking ‚Üí status is on whiteboard\n",
        "\n",
        "Then‚Ä¶ power goes off (restart)\n",
        "\n",
        "üí• Whiteboard gets erased ‚Üí you don‚Äôt know:\n",
        "\n",
        "which orders existed\n",
        "\n",
        "what was already cooked\n",
        "\n",
        "what is still pending\n",
        "That‚Äôs exactly what happens when state lives in RAM.\n",
        "\n",
        "‚úÖ Good system (external truth)\n",
        "\n",
        "Instead, you write orders in a notebook (database).\n",
        "\n",
        "Power goes off? No problem.\n",
        "You open the notebook and continue.\n",
        "\n",
        "That‚Äôs Externalized State + Stateless Service.\n",
        "\n",
        "Now let‚Äôs connect this to your FastAPI/Colab project\n",
        "1) Stateless Service Pattern\n",
        "Meaning (easy)\n",
        "\n",
        "The service should behave like:\n",
        "\n",
        "‚ÄúI can be killed and restarted at any time and I still work.‚Äù\n",
        "\n",
        "How?\n",
        "\n",
        "No jobs = {} global dictionary holding truth\n",
        "\n",
        "When request comes, read data from DB\n",
        "\n",
        "Update DB\n",
        "\n",
        "Return response\n",
        "\n",
        "‚úÖ In your notebook:\n",
        "We never stored job truth in a Python global variable.\n",
        "\n",
        "2) Externalized State Pattern\n",
        "Meaning\n",
        "\n",
        "Store the truth somewhere outside the app process:\n",
        "\n",
        "DB (SQLite/Postgres)\n",
        "\n",
        "Redis (for fast state)\n",
        "\n",
        "Queue/log (Kafka/SQS)\n",
        "\n",
        "‚úÖ In your notebook:\n",
        "\n",
        "Tables: jobs, job_steps, job_events, job_snapshots\n",
        "\n",
        "Even if runtime restarts, SQLite file still exists\n",
        "\n",
        "3) Entity Decomposition Pattern\n",
        "Meaning (easy)\n",
        "\n",
        "Don‚Äôt put everything into one big blob.\n",
        "\n",
        "Instead split into smaller pieces (entities), like:\n",
        "\n",
        "Job = the overall work\n",
        "\n",
        "Steps = the parts of work\n",
        "\n",
        "Events = the history of changes\n",
        "\n",
        "Snapshots = a saved checkpoint\n",
        "\n",
        "‚úÖ Why it matters?\n",
        "If ‚Äúextract step‚Äù fails, you can re-run only extract.\n",
        "If everything is one big JSON, it becomes messy.\n",
        "\n",
        "‚úÖ In project:\n",
        "\n",
        "jobs table ‚Üí job info\n",
        "\n",
        "job_steps table ‚Üí step-level tracking\n",
        "\n",
        "job_events table ‚Üí history\n",
        "\n",
        "job_snapshots table ‚Üí checkpoint\n",
        "\n",
        "4) Append-Only / Audit Log Pattern (Most important!)\n",
        "Meaning (easy)\n",
        "\n",
        "Instead of changing the past, you only add new facts.\n",
        "\n",
        "Like a bank statement:\n",
        "\n",
        "You don‚Äôt erase transactions\n",
        "\n",
        "You only add new entries\n",
        "\n",
        "‚úÖ Example\n",
        "Instead of updating status = RUNNING ‚Üí DONE and losing history,\n",
        "you add events:\n",
        "\n",
        "JOB_CREATED\n",
        "\n",
        "JOB_STARTED\n",
        "\n",
        "STEP_COMPLETED (INGEST)\n",
        "\n",
        "STEP_COMPLETED (EXTRACT)\n",
        "\n",
        "‚úÖ In project:\n",
        "append_event() only does INSERT into job_events.\n",
        "\n",
        "This gives you:\n",
        "\n",
        "full history\n",
        "\n",
        "audit\n",
        "\n",
        "debugging\n",
        "\n",
        "replay ability\n",
        "\n",
        "5) Replay / Reprocessing Pattern\n",
        "Meaning (easy)\n",
        "\n",
        "If you have the full event history, you can rebuild the current state anytime.\n",
        "\n",
        "Like: ‚ÄúIf I re-read my bank statement from the top, I can compute my balance.‚Äù\n",
        "\n",
        "‚úÖ In project:\n",
        "replay_job(job_id) does:\n",
        "\n",
        "read all events for that job\n",
        "\n",
        "start with empty state\n",
        "\n",
        "apply events one by one\n",
        "\n",
        "rebuild status + steps\n",
        "\n",
        "So after restart, system can ‚Äúremember‚Äù by replaying.\n",
        "\n",
        "6) Memento Pattern (Snapshots)\n",
        "Meaning (easy)\n",
        "\n",
        "Replay can be slow if you have 1 million events.\n",
        "\n",
        "So sometimes you save a checkpoint:\n",
        "\n",
        "‚ÄúAt event #5000, state looked like this.‚Äù\n",
        "\n",
        "Then to rebuild:\n",
        "\n",
        "load snapshot\n",
        "\n",
        "apply only events after snapshot\n",
        "\n",
        "‚úÖ In project:\n",
        "create_snapshot(job_id) saves the replay result into job_snapshots\n",
        "\n",
        "This is like a video game save point üéÆ\n",
        "\n",
        "7) Versioned Data Pattern\n",
        "Meaning (easy)\n",
        "\n",
        "Your system changes over time:\n",
        "\n",
        "new fields\n",
        "\n",
        "new step names\n",
        "\n",
        "new payload structure\n",
        "\n",
        "Old jobs shouldn‚Äôt break.\n",
        "\n",
        "So we store schema_version.\n",
        "\n",
        "‚úÖ In the earlier FastAPI repo version, we included schema_version.\n",
        "In the Colab mini version, we kept it simple (but we can add it easily).\n",
        "\n",
        "8) Canonical Data Model Pattern\n",
        "Meaning (easy)\n",
        "\n",
        "If multiple services read/write job info, they must agree on the same structure.\n",
        "\n",
        "Otherwise one service says:\n",
        "\n",
        "status=\"DONE\"\n",
        "\n",
        "another service uses:\n",
        "\n",
        "status=\"COMPLETED\"\n",
        "\n",
        "üí• breaks replay & integration.\n",
        "\n",
        "Canonical model = one shared truth structure.\n",
        "\n",
        "9) Soft Delete Pattern\n",
        "Meaning (easy)\n",
        "\n",
        "Never permanently delete truth.\n",
        "\n",
        "Instead:\n",
        "\n",
        "mark as deleted\n",
        "\n",
        "keep the history/events\n",
        "\n",
        "‚úÖ Why?\n",
        "\n",
        "audit required (banks, enterprise)\n",
        "\n",
        "debugging\n",
        "\n",
        "replay still possible\n",
        "\n",
        "‚úÖ In project:\n",
        "soft_delete(job_id) updates deleted_at instead of deleting rows.\n",
        "\n",
        "Why interviewers LOVE Group 3\n",
        "\n",
        "Because this is what breaks production systems:\n",
        "\n",
        "‚úÖ ‚ÄúWe had jobs running‚Ä¶ then deploy happened‚Ä¶ and everything vanished.‚Äù\n",
        "\n",
        "So when you answer:\n",
        "\n",
        "‚ÄúWe use externalized state + append-only events + replay + snapshots‚Äù\n",
        "\n",
        "They immediately think:\n",
        "‚úÖ This person builds real systems.\n",
        "\n",
        "A super simple ‚Äúrestart test‚Äù you can remember\n",
        "Step A: create job + complete steps\n",
        "\n",
        "Events:\n",
        "\n",
        "JOB_CREATED\n",
        "\n",
        "JOB_STARTED\n",
        "\n",
        "STEP_COMPLETED INGEST\n",
        "\n",
        "STEP_COMPLETED EXTRACT\n",
        "\n",
        "Step B: restart app / runtime\n",
        "\n",
        "Memory wiped\n",
        "\n",
        "Step C: replay reads events and rebuilds state\n",
        "\n",
        "Result:\n",
        "\n",
        "status = RUNNING\n",
        "\n",
        "steps = {INGEST:DONE, EXTRACT:DONE}\n",
        "\n",
        "‚úÖ truth survived restart."
      ],
      "metadata": {
        "id": "rdUUnwmjk2py"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† Pattern Mapping (You SHOULD say this in interviews)\n",
        "Pattern\tWhere shown\n",
        "Stateless Service\tNo global variables\n",
        "Externalized State\tSQLite\n",
        "Append-Only Log\tjob_events\n",
        "Replay\treplay_job()\n",
        "Memento\tjob_snapshots\n",
        "Soft Delete\tdeleted_at\n",
        "Survive Restart\tRuntime restart demo\n",
        "üöÄ Next (Strongly Recommended)\n",
        "\n",
        "I can next:\n",
        "1Ô∏è‚É£ Convert this into Colab ‚Üí FastAPI server running inside notebook\n",
        "2Ô∏è‚É£ Add RAG / Agent pipeline example using same patterns\n",
        "3Ô∏è‚É£ Add diagram + interview Q&A sheet\n",
        "4Ô∏è‚É£ Merge Group 2 + Group 3 into one project\n",
        "\n",
        "Just tell me the number üëç\n",
        "\n",
        "could you explain ore in depth and easy to understand\n",
        "\n",
        "Of course, Bobikutty üôÇ ‚Äî let‚Äôs make this super clear, slow, and deep, but still easy. I‚Äôll explain using a simple story first, then map every pattern to what you saw in the Colab project.\n",
        "\n",
        "The Big Idea (Group 3) in One Sentence\n",
        "\n",
        "Your app must never store ‚Äútruth‚Äù only in memory, because memory disappears on restart.\n",
        "\n",
        "So we design the system so that:\n",
        "\n",
        "memory can be empty any time,\n",
        "\n",
        "and the system can still rebuild the truth from storage.\n",
        "\n",
        "A Simple Story (Best way to remember)\n",
        "\n",
        "Imagine you are running a restaurant kitchen.\n",
        "\n",
        "‚ùå Bad system (in-memory truth)\n",
        "\n",
        "You write all orders on a whiteboard.\n",
        "\n",
        "Customer order comes ‚Üí you write on whiteboard\n",
        "\n",
        "Kitchen is cooking ‚Üí status is on whiteboard\n",
        "\n",
        "Then‚Ä¶ power goes off (restart)\n",
        "\n",
        "üí• Whiteboard gets erased ‚Üí you don‚Äôt know:\n",
        "\n",
        "which orders existed\n",
        "\n",
        "what was already cooked\n",
        "\n",
        "what is still pending\n",
        "That‚Äôs exactly what happens when state lives in RAM.\n",
        "\n",
        "‚úÖ Good system (external truth)\n",
        "\n",
        "Instead, you write orders in a notebook (database).\n",
        "\n",
        "Power goes off? No problem.\n",
        "You open the notebook and continue.\n",
        "\n",
        "That‚Äôs Externalized State + Stateless Service.\n",
        "\n",
        "Now let‚Äôs connect this to your FastAPI/Colab project\n",
        "1) Stateless Service Pattern\n",
        "Meaning (easy)\n",
        "\n",
        "The service should behave like:\n",
        "\n",
        "‚ÄúI can be killed and restarted at any time and I still work.‚Äù\n",
        "\n",
        "How?\n",
        "\n",
        "No jobs = {} global dictionary holding truth\n",
        "\n",
        "When request comes, read data from DB\n",
        "\n",
        "Update DB\n",
        "\n",
        "Return response\n",
        "\n",
        "‚úÖ In your notebook:\n",
        "We never stored job truth in a Python global variable.\n",
        "\n",
        "2) Externalized State Pattern\n",
        "Meaning\n",
        "\n",
        "Store the truth somewhere outside the app process:\n",
        "\n",
        "DB (SQLite/Postgres)\n",
        "\n",
        "Redis (for fast state)\n",
        "\n",
        "Queue/log (Kafka/SQS)\n",
        "\n",
        "‚úÖ In your notebook:\n",
        "\n",
        "Tables: jobs, job_steps, job_events, job_snapshots\n",
        "\n",
        "Even if runtime restarts, SQLite file still exists\n",
        "\n",
        "3) Entity Decomposition Pattern\n",
        "Meaning (easy)\n",
        "\n",
        "Don‚Äôt put everything into one big blob.\n",
        "\n",
        "Instead split into smaller pieces (entities), like:\n",
        "\n",
        "Job = the overall work\n",
        "\n",
        "Steps = the parts of work\n",
        "\n",
        "Events = the history of changes\n",
        "\n",
        "Snapshots = a saved checkpoint\n",
        "\n",
        "‚úÖ Why it matters?\n",
        "If ‚Äúextract step‚Äù fails, you can re-run only extract.\n",
        "If everything is one big JSON, it becomes messy.\n",
        "\n",
        "‚úÖ In project:\n",
        "\n",
        "jobs table ‚Üí job info\n",
        "\n",
        "job_steps table ‚Üí step-level tracking\n",
        "\n",
        "job_events table ‚Üí history\n",
        "\n",
        "job_snapshots table ‚Üí checkpoint\n",
        "\n",
        "4) Append-Only / Audit Log Pattern (Most important!)\n",
        "Meaning (easy)\n",
        "\n",
        "Instead of changing the past, you only add new facts.\n",
        "\n",
        "Like a bank statement:\n",
        "\n",
        "You don‚Äôt erase transactions\n",
        "\n",
        "You only add new entries\n",
        "\n",
        "‚úÖ Example\n",
        "Instead of updating status = RUNNING ‚Üí DONE and losing history,\n",
        "you add events:\n",
        "\n",
        "JOB_CREATED\n",
        "\n",
        "JOB_STARTED\n",
        "\n",
        "STEP_COMPLETED (INGEST)\n",
        "\n",
        "STEP_COMPLETED (EXTRACT)\n",
        "\n",
        "‚úÖ In project:\n",
        "append_event() only does INSERT into job_events.\n",
        "\n",
        "This gives you:\n",
        "\n",
        "full history\n",
        "\n",
        "audit\n",
        "\n",
        "debugging\n",
        "\n",
        "replay ability\n",
        "\n",
        "5) Replay / Reprocessing Pattern\n",
        "Meaning (easy)\n",
        "\n",
        "If you have the full event history, you can rebuild the current state anytime.\n",
        "\n",
        "Like: ‚ÄúIf I re-read my bank statement from the top, I can compute my balance.‚Äù\n",
        "\n",
        "‚úÖ In project:\n",
        "replay_job(job_id) does:\n",
        "\n",
        "read all events for that job\n",
        "\n",
        "start with empty state\n",
        "\n",
        "apply events one by one\n",
        "\n",
        "rebuild status + steps\n",
        "\n",
        "So after restart, system can ‚Äúremember‚Äù by replaying.\n",
        "\n",
        "6) Memento Pattern (Snapshots)\n",
        "Meaning (easy)\n",
        "\n",
        "Replay can be slow if you have 1 million events.\n",
        "\n",
        "So sometimes you save a checkpoint:\n",
        "\n",
        "‚ÄúAt event #5000, state looked like this.‚Äù\n",
        "\n",
        "Then to rebuild:\n",
        "\n",
        "load snapshot\n",
        "\n",
        "apply only events after snapshot\n",
        "\n",
        "‚úÖ In project:\n",
        "create_snapshot(job_id) saves the replay result into job_snapshots\n",
        "\n",
        "This is like a video game save point üéÆ\n",
        "\n",
        "7) Versioned Data Pattern\n",
        "Meaning (easy)\n",
        "\n",
        "Your system changes over time:\n",
        "\n",
        "new fields\n",
        "\n",
        "new step names\n",
        "\n",
        "new payload structure\n",
        "\n",
        "Old jobs shouldn‚Äôt break.\n",
        "\n",
        "So we store schema_version.\n",
        "\n",
        "‚úÖ In the earlier FastAPI repo version, we included schema_version.\n",
        "In the Colab mini version, we kept it simple (but we can add it easily).\n",
        "\n",
        "8) Canonical Data Model Pattern\n",
        "Meaning (easy)\n",
        "\n",
        "If multiple services read/write job info, they must agree on the same structure.\n",
        "\n",
        "Otherwise one service says:\n",
        "\n",
        "status=\"DONE\"\n",
        "\n",
        "another service uses:\n",
        "\n",
        "status=\"COMPLETED\"\n",
        "\n",
        "üí• breaks replay & integration.\n",
        "\n",
        "Canonical model = one shared truth structure.\n",
        "\n",
        "9) Soft Delete Pattern\n",
        "Meaning (easy)\n",
        "\n",
        "Never permanently delete truth.\n",
        "\n",
        "Instead:\n",
        "\n",
        "mark as deleted\n",
        "\n",
        "keep the history/events\n",
        "\n",
        "‚úÖ Why?\n",
        "\n",
        "audit required (banks, enterprise)\n",
        "\n",
        "debugging\n",
        "\n",
        "replay still possible\n",
        "\n",
        "‚úÖ In project:\n",
        "soft_delete(job_id) updates deleted_at instead of deleting rows.\n",
        "\n",
        "Why interviewers LOVE Group 3\n",
        "\n",
        "Because this is what breaks production systems:\n",
        "\n",
        "‚úÖ ‚ÄúWe had jobs running‚Ä¶ then deploy happened‚Ä¶ and everything vanished.‚Äù\n",
        "\n",
        "So when you answer:\n",
        "\n",
        "‚ÄúWe use externalized state + append-only events + replay + snapshots‚Äù\n",
        "\n",
        "They immediately think:\n",
        "‚úÖ This person builds real systems.\n",
        "\n",
        "A super simple ‚Äúrestart test‚Äù you can remember\n",
        "Step A: create job + complete steps\n",
        "\n",
        "Events:\n",
        "\n",
        "JOB_CREATED\n",
        "\n",
        "JOB_STARTED\n",
        "\n",
        "STEP_COMPLETED INGEST\n",
        "\n",
        "STEP_COMPLETED EXTRACT\n",
        "\n",
        "Step B: restart app / runtime\n",
        "\n",
        "Memory wiped\n",
        "\n",
        "Step C: replay reads events and rebuilds state\n",
        "\n",
        "Result:\n",
        "\n",
        "status = RUNNING\n",
        "\n",
        "steps = {INGEST:DONE, EXTRACT:DONE}\n",
        "\n",
        "‚úÖ truth survived restart.\n",
        "\n",
        "Want me to upgrade your Colab into a ‚Äúreal FastAPI server inside Colab‚Äù?\n",
        "\n",
        "Right now your Colab is ‚Äúservice functions only‚Äù.\n",
        "Next step is to run FastAPI endpoints inside Colab (with uvicorn + ngrok) so it feels like a real API.\n",
        "\n",
        "If you say ‚Äúyes make it real FastAPI in Colab‚Äù, I‚Äôll give you the notebook cells for that too.\n",
        "\n",
        "yes\n",
        "\n",
        "Perfect ‚úÖ Bobikutty ‚Äî here‚Äôs the ‚Äúreal FastAPI server inside Google Colab‚Äù version of your Group-3 project.\n",
        "\n",
        "This notebook will:\n",
        "\n",
        "start a real FastAPI API\n",
        "\n",
        "expose it with a public URL using ngrok\n",
        "\n",
        "persist state in SQLite file\n",
        "\n",
        "let you restart the runtime and still replay state (proof that memory isn‚Äôt required)\n",
        "\n",
        "Copy these cells in order into Colab.\n",
        "\n",
        "‚úÖ Colab Notebook: Group 3 FastAPI Server + SQLite + Replay + Snapshot + Soft Delete\n",
        "üß© Cell 1 ‚Äî Install dependencies\n",
        "!pip -q install fastapi uvicorn nest-asyncio pyngrok pydantic\n",
        "\n",
        "üß© Cell 2 ‚Äî Create the full FastAPI app file (app.py)\n",
        "%%writefile app.py\n",
        "import sqlite3\n",
        "import json\n",
        "import uuid\n",
        "from datetime import datetime, timezone\n",
        "from typing import Dict, Any, Optional, Tuple\n",
        "\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "DB_PATH = \"group3_state.db\"\n",
        "DEFAULT_STEPS = [\"INGEST\", \"EXTRACT\", \"EMBED\", \"INDEX\"]\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities\n",
        "# ---------------------------\n",
        "def utcnow() -> str:\n",
        "    return datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "def get_conn():\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    conn.row_factory = sqlite3.Row\n",
        "    conn.execute(\"PRAGMA foreign_keys=ON;\")\n",
        "    conn.execute(\"PRAGMA journal_mode=WAL;\")\n",
        "    return conn\n",
        "\n",
        "def init_db():\n",
        "    conn = get_conn()\n",
        "\n",
        "    conn.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS jobs(\n",
        "        job_id TEXT PRIMARY KEY,\n",
        "        job_type TEXT NOT NULL,\n",
        "        status TEXT NOT NULL,\n",
        "        schema_version INTEGER NOT NULL,\n",
        "        deleted_at TEXT NULL,\n",
        "        created_at TEXT NOT NULL,\n",
        "        updated_at TEXT NOT NULL\n",
        "    );\n",
        "    \"\"\")\n",
        "\n",
        "    conn.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS job_steps(\n",
        "        step_id TEXT PRIMARY KEY,\n",
        "        job_id TEXT NOT NULL,\n",
        "        step_name TEXT NOT NULL,\n",
        "        status TEXT NOT NULL,\n",
        "        created_at TEXT NOT NULL,\n",
        "        updated_at TEXT NOT NULL,\n",
        "        UNIQUE(job_id, step_name),\n",
        "        FOREIGN KEY(job_id) REFERENCES jobs(job_id)\n",
        "    );\n",
        "    \"\"\")\n",
        "\n",
        "    # Append-only audit log (INSERT only)\n",
        "    conn.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS job_events(\n",
        "        event_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        job_id TEXT NOT NULL,\n",
        "        event_type TEXT NOT NULL,\n",
        "        schema_version INTEGER NOT NULL,\n",
        "        payload_json TEXT NOT NULL,\n",
        "        created_at TEXT NOT NULL,\n",
        "        FOREIGN KEY(job_id) REFERENCES jobs(job_id)\n",
        "    );\n",
        "    \"\"\")\n",
        "\n",
        "    # Memento snapshots\n",
        "    conn.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS job_snapshots(\n",
        "        snapshot_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        job_id TEXT NOT NULL,\n",
        "        snapshot_json TEXT NOT NULL,\n",
        "        created_at TEXT NOT NULL,\n",
        "        FOREIGN KEY(job_id) REFERENCES jobs(job_id)\n",
        "    );\n",
        "    \"\"\")\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "# ---------------------------\n",
        "# Canonical statuses\n",
        "# ---------------------------\n",
        "JOB_CREATED = \"CREATED\"\n",
        "JOB_RUNNING = \"RUNNING\"\n",
        "JOB_DONE = \"DONE\"\n",
        "JOB_FAILED = \"FAILED\"\n",
        "\n",
        "STEP_PENDING = \"PENDING\"\n",
        "STEP_DONE = \"DONE\"\n",
        "\n",
        "# ---------------------------\n",
        "# Schemas\n",
        "# ---------------------------\n",
        "class CreateJobRequest(BaseModel):\n",
        "    job_type: str = Field(default=\"PDF_EXTRACT\")\n",
        "    schema_version: int = Field(default=1, ge=1)\n",
        "\n",
        "class JobView(BaseModel):\n",
        "    job_id: str\n",
        "    job_type: str\n",
        "    status: str\n",
        "    schema_version: int\n",
        "    deleted_at: Optional[str] = None\n",
        "    created_at: str\n",
        "    updated_at: str\n",
        "\n",
        "class StepView(BaseModel):\n",
        "    step_id: str\n",
        "    job_id: str\n",
        "    step_name: str\n",
        "    status: str\n",
        "    created_at: str\n",
        "    updated_at: str\n",
        "\n",
        "class JobWithSteps(BaseModel):\n",
        "    job: JobView\n",
        "    steps: list[StepView]\n",
        "\n",
        "class ReplayResult(BaseModel):\n",
        "    job_id: str\n",
        "    rebuilt_status: Optional[str]\n",
        "    steps: Dict[str, str]\n",
        "    applied_events: int\n",
        "    from_snapshot: bool\n",
        "\n",
        "class SnapshotResult(BaseModel):\n",
        "    job_id: str\n",
        "    snapshot_created_at: str\n",
        "\n",
        "# ---------------------------\n",
        "# Event Store (Append-only)\n",
        "# ---------------------------\n",
        "def append_event(job_id: str, event_type: str, schema_version: int, payload: Dict[str, Any]) -> int:\n",
        "    conn = get_conn()\n",
        "    payload_json = json.dumps(payload, separators=(\",\", \":\"), ensure_ascii=False)\n",
        "    cur = conn.execute(\n",
        "        \"\"\"\n",
        "        INSERT INTO job_events(job_id, event_type, schema_version, payload_json, created_at)\n",
        "        VALUES (?, ?, ?, ?, ?)\n",
        "        \"\"\",\n",
        "        (job_id, event_type, schema_version, payload_json, utcnow())\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    return int(cur.lastrowid)\n",
        "\n",
        "def list_events(job_id: str):\n",
        "    conn = get_conn()\n",
        "    rows = conn.execute(\n",
        "        \"SELECT * FROM job_events WHERE job_id=? ORDER BY event_id ASC\",\n",
        "        (job_id,)\n",
        "    ).fetchall()\n",
        "    conn.close()\n",
        "    out = []\n",
        "    for r in rows:\n",
        "        out.append({\n",
        "            \"event_id\": r[\"event_id\"],\n",
        "            \"event_type\": r[\"event_type\"],\n",
        "            \"schema_version\": r[\"schema_version\"],\n",
        "            \"payload\": json.loads(r[\"payload_json\"]),\n",
        "            \"created_at\": r[\"created_at\"]\n",
        "        })\n",
        "    return out\n",
        "\n",
        "def save_snapshot(job_id: str, snapshot: Dict[str, Any]) -> str:\n",
        "    conn = get_conn()\n",
        "    created_at = utcnow()\n",
        "    conn.execute(\n",
        "        \"INSERT INTO job_snapshots(job_id, snapshot_json, created_at) VALUES (?, ?, ?)\",\n",
        "        (job_id, json.dumps(snapshot, separators=(\",\", \":\"), ensure_ascii=False), created_at)\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    return created_at\n",
        "\n",
        "def get_latest_snapshot(job_id: str) -> Optional[Dict[str, Any]]:\n",
        "    conn = get_conn()\n",
        "    row = conn.execute(\n",
        "        \"\"\"\n",
        "        SELECT snapshot_json, created_at\n",
        "        FROM job_snapshots\n",
        "        WHERE job_id=?\n",
        "        ORDER BY snapshot_id DESC\n",
        "        LIMIT 1\n",
        "        \"\"\",\n",
        "        (job_id,)\n",
        "    ).fetchone()\n",
        "    conn.close()\n",
        "    if not row:\n",
        "        return None\n",
        "    snap = json.loads(row[\"snapshot_json\"])\n",
        "    snap[\"_snapshot_created_at\"] = row[\"created_at\"]\n",
        "    return snap\n",
        "\n",
        "# ---------------------------\n",
        "# Replay Engine\n",
        "# ---------------------------\n",
        "def apply_event(state: Dict[str, Any], event_type: str, payload: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    if event_type == \"JOB_CREATED\":\n",
        "        state[\"status\"] = JOB_CREATED\n",
        "        state[\"steps\"] = {}\n",
        "        state[\"deleted\"] = False\n",
        "    elif event_type == \"JOB_STARTED\":\n",
        "        state[\"status\"] = JOB_RUNNING\n",
        "    elif event_type == \"STEP_CREATED\":\n",
        "        step = payload[\"step_name\"]\n",
        "        state[\"steps\"][step] = STEP_PENDING\n",
        "    elif event_type == \"STEP_COMPLETED\":\n",
        "        step = payload[\"step_name\"]\n",
        "        state[\"steps\"][step] = STEP_DONE\n",
        "    elif event_type == \"JOB_COMPLETED\":\n",
        "        state[\"status\"] = JOB_DONE\n",
        "    elif event_type == \"JOB_FAILED\":\n",
        "        state[\"status\"] = JOB_FAILED\n",
        "    elif event_type == \"JOB_SOFT_DELETED\":\n",
        "        state[\"deleted\"] = True\n",
        "    return state\n",
        "\n",
        "def replay_job(job_id: str) -> Tuple[Dict[str, Any], int, bool]:\n",
        "    snapshot = get_latest_snapshot(job_id)\n",
        "    from_snapshot = snapshot is not None\n",
        "\n",
        "    if snapshot:\n",
        "        state = {\n",
        "            \"status\": snapshot.get(\"status\"),\n",
        "            \"steps\": snapshot.get(\"steps\", {}),\n",
        "            \"deleted\": snapshot.get(\"deleted\", False),\n",
        "        }\n",
        "        last_event_id = int(snapshot.get(\"last_event_id\", 0))\n",
        "    else:\n",
        "        state = {\"status\": None, \"steps\": {}, \"deleted\": False}\n",
        "        last_event_id = 0\n",
        "\n",
        "    events = list_events(job_id)\n",
        "    applied = 0\n",
        "    for e in events:\n",
        "        if e[\"event_id\"] <= last_event_id:\n",
        "            continue\n",
        "        state = apply_event(state, e[\"event_type\"], e[\"payload\"])\n",
        "        applied += 1\n",
        "\n",
        "    state[\"last_event_id\"] = events[-1][\"event_id\"] if events else last_event_id\n",
        "    return state, applied, from_snapshot\n",
        "\n",
        "# ---------------------------\n",
        "# Services (Stateless)\n",
        "# ---------------------------\n",
        "def create_job(job_type: str, schema_version: int) -> str:\n",
        "    job_id = str(uuid.uuid4())\n",
        "    now = utcnow()\n",
        "\n",
        "    conn = get_conn()\n",
        "    conn.execute(\n",
        "        \"\"\"\n",
        "        INSERT INTO jobs(job_id, job_type, status, schema_version, deleted_at, created_at, updated_at)\n",
        "        VALUES (?, ?, ?, ?, NULL, ?, ?)\n",
        "        \"\"\",\n",
        "        (job_id, job_type, JOB_CREATED, schema_version, now, now)\n",
        "    )\n",
        "\n",
        "    # Steps are separate entity (Entity Decomposition)\n",
        "    for step in DEFAULT_STEPS:\n",
        "        step_id = str(uuid.uuid4())\n",
        "        conn.execute(\n",
        "            \"\"\"\n",
        "            INSERT INTO job_steps(step_id, job_id, step_name, status, created_at, updated_at)\n",
        "            VALUES (?, ?, ?, ?, ?, ?)\n",
        "            \"\"\",\n",
        "            (step_id, job_id, step, STEP_PENDING, now, now)\n",
        "        )\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    append_event(job_id, \"JOB_CREATED\", schema_version, {\"job_type\": job_type})\n",
        "    for step in DEFAULT_STEPS:\n",
        "        append_event(job_id, \"STEP_CREATED\", schema_version, {\"step_name\": step})\n",
        "\n",
        "    return job_id\n",
        "\n",
        "def get_job_with_steps(job_id: str) -> Optional[Dict[str, Any]]:\n",
        "    conn = get_conn()\n",
        "    job = conn.execute(\"SELECT * FROM jobs WHERE job_id=?\", (job_id,)).fetchone()\n",
        "    if not job:\n",
        "        conn.close()\n",
        "        return None\n",
        "    steps = conn.execute(\n",
        "        \"SELECT * FROM job_steps WHERE job_id=? ORDER BY step_name ASC\",\n",
        "        (job_id,)\n",
        "    ).fetchall()\n",
        "    conn.close()\n",
        "    return {\"job\": dict(job), \"steps\": [dict(s) for s in steps]}\n",
        "\n",
        "def start_job(job_id: str) -> None:\n",
        "    conn = get_conn()\n",
        "    job = conn.execute(\"SELECT schema_version, deleted_at FROM jobs WHERE job_id=?\", (job_id,)).fetchone()\n",
        "    if not job:\n",
        "        conn.close()\n",
        "        raise HTTPException(status_code=404, detail=\"Job not found\")\n",
        "    if job[\"deleted_at\"] is not None:\n",
        "        conn.close()\n",
        "        raise HTTPException(status_code=410, detail=\"Job is soft-deleted\")\n",
        "\n",
        "    now = utcnow()\n",
        "    conn.execute(\"UPDATE jobs SET status=?, updated_at=? WHERE job_id=?\", (JOB_RUNNING, now, job_id))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    append_event(job_id, \"JOB_STARTED\", int(job[\"schema_version\"]), {})\n",
        "\n",
        "def complete_step(job_id: str, step_name: str) -> None:\n",
        "    step_name = step_name.upper()\n",
        "\n",
        "    conn = get_conn()\n",
        "    job = conn.execute(\"SELECT schema_version, deleted_at FROM jobs WHERE job_id=?\", (job_id,)).fetchone()\n",
        "    if not job:\n",
        "        conn.close()\n",
        "        raise HTTPException(status_code=404, detail=\"Job not found\")\n",
        "    if job[\"deleted_at\"] is not None:\n",
        "        conn.close()\n",
        "        raise HTTPException(status_code=410, detail=\"Job is soft-deleted\")\n",
        "\n",
        "    step = conn.execute(\n",
        "        \"SELECT * FROM job_steps WHERE job_id=? AND step_name=?\",\n",
        "        (job_id, step_name)\n",
        "    ).fetchone()\n",
        "    if not step:\n",
        "        conn.close()\n",
        "        raise HTTPException(status_code=404, detail=f\"Step not found: {step_name}\")\n",
        "\n",
        "    now = utcnow()\n",
        "    conn.execute(\n",
        "        \"UPDATE job_steps SET status=?, updated_at=? WHERE job_id=? AND step_name=?\",\n",
        "        (STEP_DONE, now, job_id, step_name)\n",
        "    )\n",
        "    conn.execute(\"UPDATE jobs SET updated_at=? WHERE job_id=?\", (now, job_id))\n",
        "\n",
        "    remaining = conn.execute(\n",
        "        \"SELECT COUNT(*) AS c FROM job_steps WHERE job_id=? AND status!=?\",\n",
        "        (job_id, STEP_DONE)\n",
        "    ).fetchone()[\"c\"]\n",
        "\n",
        "    if remaining == 0:\n",
        "        conn.execute(\"UPDATE jobs SET status=?, updated_at=? WHERE job_id=?\", (JOB_DONE, now, job_id))\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    sv = int(job[\"schema_version\"])\n",
        "    append_event(job_id, \"STEP_COMPLETED\", sv, {\"step_name\": step_name})\n",
        "    if remaining == 0:\n",
        "        append_event(job_id, \"JOB_COMPLETED\", sv, {})\n",
        "\n",
        "def soft_delete_job(job_id: str) -> None:\n",
        "    conn = get_conn()\n",
        "    job = conn.execute(\"SELECT schema_version FROM jobs WHERE job_id=?\", (job_id,)).fetchone()\n",
        "    if not job:\n",
        "        conn.close()\n",
        "        raise HTTPException(status_code=404, detail=\"Job not found\")\n",
        "\n",
        "    now = utcnow()\n",
        "    conn.execute(\n",
        "        \"UPDATE jobs SET deleted_at=?, updated_at=? WHERE job_id=? AND deleted_at IS NULL\",\n",
        "        (now, now, job_id)\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    append_event(job_id, \"JOB_SOFT_DELETED\", int(job[\"schema_version\"]), {\"deleted_at\": now})\n",
        "\n",
        "def create_snapshot(job_id: str) -> str:\n",
        "    state, _, _ = replay_job(job_id)\n",
        "    snapshot = {\n",
        "        \"status\": state.get(\"status\"),\n",
        "        \"steps\": state.get(\"steps\", {}),\n",
        "        \"deleted\": state.get(\"deleted\", False),\n",
        "        \"last_event_id\": state.get(\"last_event_id\", 0),\n",
        "    }\n",
        "    return save_snapshot(job_id, snapshot)\n",
        "\n",
        "# ---------------------------\n",
        "# FastAPI app\n",
        "# ---------------------------\n",
        "app = FastAPI(title=\"Group 3 ‚Äî State Survives Restarts (Colab)\")\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "def on_startup():\n",
        "    init_db()\n",
        "\n",
        "@app.post(\"/jobs\", response_model=JobView)\n",
        "def api_create_job(req: CreateJobRequest):\n",
        "    job_id = create_job(req.job_type, req.schema_version)\n",
        "    data = get_job_with_steps(job_id)\n",
        "    return data[\"job\"]\n",
        "\n",
        "@app.get(\"/jobs/{job_id}\", response_model=JobWithSteps)\n",
        "def api_get_job(job_id: str):\n",
        "    data = get_job_with_steps(job_id)\n",
        "    if not data:\n",
        "        raise HTTPException(status_code=404, detail=\"Job not found\")\n",
        "    return {\"job\": data[\"job\"], \"steps\": data[\"steps\"]}\n",
        "\n",
        "@app.post(\"/jobs/{job_id}/start\")\n",
        "def api_start(job_id: str):\n",
        "    start_job(job_id)\n",
        "    return {\"ok\": True}\n",
        "\n",
        "@app.post(\"/jobs/{job_id}/steps/{step_name}/complete\")\n",
        "def api_complete_step(job_id: str, step_name: str):\n",
        "    complete_step(job_id, step_name)\n",
        "    return {\"ok\": True}\n",
        "\n",
        "@app.post(\"/jobs/{job_id}/snapshot\", response_model=SnapshotResult)\n",
        "def api_snapshot(job_id: str):\n",
        "    # validate exists\n",
        "    if not get_job_with_steps(job_id):\n",
        "        raise HTTPException(status_code=404, detail=\"Job not found\")\n",
        "    created_at = create_snapshot(job_id)\n",
        "    return {\"job_id\": job_id, \"snapshot_created_at\": created_at}\n",
        "\n",
        "@app.post(\"/jobs/{job_id}/replay\", response_model=ReplayResult)\n",
        "def api_replay(job_id: str):\n",
        "    if not get_job_with_steps(job_id):\n",
        "        raise HTTPException(status_code=404, detail=\"Job not found\")\n",
        "    state, applied, from_snapshot = replay_job(job_id)\n",
        "    return {\n",
        "        \"job_id\": job_id,\n",
        "        \"rebuilt_status\": state.get(\"status\"),\n",
        "        \"steps\": state.get(\"steps\", {}),\n",
        "        \"applied_events\": applied,\n",
        "        \"from_snapshot\": from_snapshot\n",
        "    }\n",
        "\n",
        "@app.delete(\"/jobs/{job_id}\")\n",
        "def api_delete(job_id: str):\n",
        "    soft_delete_job(job_id)\n",
        "    return {\"ok\": True}\n",
        "\n",
        "üß© Cell 3 ‚Äî Start FastAPI server in Colab + expose with ngrok\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import threading\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"‚úÖ Public API URL:\", public_url)\n",
        "\n",
        "# Run uvicorn in a background thread (still within this notebook runtime)\n",
        "def run():\n",
        "    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "\n",
        "thread = threading.Thread(target=run, daemon=True)\n",
        "thread.start()\n",
        "\n",
        "\n",
        "‚úÖ After this cell, your API is live at the printed ngrok URL.\n",
        "\n",
        "üß© Cell 4 ‚Äî Helper: easy requests from inside Colab\n",
        "import requests\n",
        "\n",
        "BASE = str(public_url)\n",
        "\n",
        "def post(path, json_body=None):\n",
        "    r = requests.post(BASE + path, json=json_body)\n",
        "    return r.status_code, r.json()\n",
        "\n",
        "def get(path):\n",
        "    r = requests.get(BASE + path)\n",
        "    return r.status_code, r.json()\n",
        "\n",
        "def delete(path):\n",
        "    r = requests.delete(BASE + path)\n",
        "    return r.status_code, r.json()\n",
        "\n",
        "üß© Cell 5 ‚Äî Demo flow (Create ‚Üí Start ‚Üí Complete steps ‚Üí Snapshot ‚Üí Replay)\n",
        "# 1) Create job\n",
        "code, job = post(\"/jobs\", {\"job_type\": \"PDF_EXTRACT\", \"schema_version\": 1})\n",
        "code, job\n",
        "\n",
        "job_id = job[\"job_id\"]\n",
        "job_id\n",
        "\n",
        "# 2) Start job\n",
        "post(f\"/jobs/{job_id}/start\")\n",
        "\n",
        "# 3) Complete some steps\n",
        "post(f\"/jobs/{job_id}/steps/INGEST/complete\")\n",
        "post(f\"/jobs/{job_id}/steps/EXTRACT/complete\")\n",
        "\n",
        "# 4) Snapshot (Memento)\n",
        "post(f\"/jobs/{job_id}/snapshot\")\n",
        "\n",
        "# 5) Replay (Reprocessing)\n",
        "post(f\"/jobs/{job_id}/replay\")\n",
        "\n",
        "# 6) Check current DB state\n",
        "get(f\"/jobs/{job_id}\")\n",
        "\n",
        "üß© Cell 6 ‚Äî Soft delete demo\n",
        "delete(f\"/jobs/{job_id}\")\n",
        "\n",
        "# Replay still works because events are kept (truth not destroyed)\n",
        "post(f\"/jobs/{job_id}/replay\")\n",
        "\n",
        "‚úÖ The ‚ÄúRestart Proof‚Äù (most important)\n",
        "Do this now:\n",
        "\n",
        "Runtime ‚Üí Restart runtime (Colab menu)\n",
        "\n",
        "Then run ONLY these cells again:\n",
        "\n",
        "Cell 3 (start server + ngrok)\n",
        "\n",
        "Cell 4 (helper requests)\n",
        "\n",
        "Then run this cell:\n",
        "\n",
        "üß© Cell 7 ‚Äî After restart: replay still works\n",
        "# Replace with your existing job_id you saved somewhere (copy it before restart)\n",
        "existing_job_id = job_id  # if you still have it; otherwise paste the string here\n",
        "\n",
        "post(f\"/jobs/{existing_job_id}/replay\")\n",
        "\n",
        "\n",
        "‚úÖ If you see the same status/steps, you‚Äôve proven:\n",
        "\n",
        "stateless service\n",
        "\n",
        "externalized state\n",
        "\n",
        "append-only audit log\n",
        "\n",
        "replay recovery\n",
        "\n",
        "snapshot support\n",
        "\n",
        "restart safe\n",
        "\n",
        "If you want it even easier (1 click restart test)\n",
        "\n",
        "Tell me and I‚Äôll add a ‚Äúsave job_id to a file + reload after restart‚Äù cell so you don‚Äôt lose the job_id variable when Colab restarts.\n",
        "\n",
        "whn sbapshts are saved\n",
        "\n",
        "Great question üëç ‚Äî this is exactly where many people get confused, so let‚Äôs slow it down and make it crystal clear.\n",
        "\n",
        "Short, clear answer first\n",
        "\n",
        "Snapshots are saved at controlled, meaningful checkpoints ‚Äî NOT on every change.\n",
        "\n",
        "They are saved intentionally, when you decide:\n",
        "\n",
        "‚ÄúThis state is important‚Äù\n",
        "\n",
        "‚ÄúReplay from the beginning would be expensive‚Äù\n",
        "\n",
        "‚ÄúI may need fast recovery from here‚Äù\n",
        "\n",
        "Think of snapshots like a video-game save point üéÆ\n",
        "\n",
        "You don‚Äôt save every second.\n",
        "You save:\n",
        "\n",
        "after finishing a level\n",
        "\n",
        "before a boss fight\n",
        "\n",
        "at a safe checkpoint\n",
        "\n",
        "In YOUR FastAPI / Colab project\n",
        "üìå When are snapshots saved?\n",
        "‚úÖ ONLY when this endpoint is called\n",
        "POST /jobs/{job_id}/snapshot\n",
        "\n",
        "\n",
        "Internally, this calls:\n",
        "\n",
        "create_snapshot(job_id)\n",
        "\n",
        "\n",
        "Which does:\n",
        "\n",
        "Replay all events to rebuild the current state\n",
        "\n",
        "Store that rebuilt state in job_snapshots\n",
        "\n",
        "So snapshots are manual / policy-driven, not automatic.\n",
        "\n",
        "Why NOT save snapshots all the time?\n",
        "‚ùå Bad idea\n",
        "\n",
        "Snapshot after every event\n",
        "\n",
        "Huge storage usage\n",
        "\n",
        "Almost same as duplicating state tables\n",
        "\n",
        "No real benefit\n",
        "\n",
        "‚úÖ Good idea\n",
        "\n",
        "Append-only events for truth\n",
        "\n",
        "Snapshots for speed\n",
        "\n",
        "Common REAL-WORLD snapshot strategies (IMPORTANT)\n",
        "1Ô∏è‚É£ Time-based snapshots\n",
        "\n",
        "‚ÄúEvery 10 minutes‚Äù\n",
        "\n",
        "Used when:\n",
        "\n",
        "jobs run very long\n",
        "\n",
        "continuous data streams\n",
        "\n",
        "Example:\n",
        "\n",
        "Snapshot every 10 minutes OR every 1,000 events\n",
        "\n",
        "2Ô∏è‚É£ Milestone-based snapshots (most common)\n",
        "\n",
        "‚ÄúWhen something meaningful finishes‚Äù\n",
        "\n",
        "Examples:\n",
        "\n",
        "After job starts\n",
        "\n",
        "After each major pipeline phase\n",
        "\n",
        "After job completes\n",
        "\n",
        "In your case:\n",
        "\n",
        "INGEST done ‚Üí snapshot\n",
        "EXTRACT done ‚Üí snapshot\n",
        "JOB DONE ‚Üí snapshot\n",
        "\n",
        "3Ô∏è‚É£ Size-based snapshots\n",
        "\n",
        "‚ÄúWhen replay cost becomes high‚Äù\n",
        "\n",
        "Example:\n",
        "\n",
        "If events > 5,000 ‚Üí snapshot\n",
        "\n",
        "4Ô∏è‚É£ Failure-based snapshots\n",
        "\n",
        "‚ÄúBefore risky operation‚Äù\n",
        "\n",
        "Example:\n",
        "\n",
        "Before calling an expensive LLM\n",
        "\n",
        "Before external API calls\n",
        "\n",
        "Before distributed fan-out\n",
        "\n",
        "What exactly is inside a snapshot?\n",
        "\n",
        "In your project, snapshot stores:\n",
        "\n",
        "{\n",
        "  \"status\": \"RUNNING\",\n",
        "  \"steps\": {\n",
        "    \"INGEST\": \"DONE\",\n",
        "    \"EXTRACT\": \"DONE\",\n",
        "    \"EMBED\": \"PENDING\"\n",
        "  },\n",
        "  \"deleted\": false,\n",
        "  \"last_event_id\": 12\n",
        "}\n",
        "\n",
        "\n",
        "üìå Key thing:\n",
        "\n",
        "last_event_id tells replay where to continue from\n",
        "\n",
        "Everything before that is already ‚Äúsummarized‚Äù\n",
        "\n",
        "How replay uses snapshots (step-by-step)\n",
        "Without snapshot\n",
        "Replay events:\n",
        "1 ‚Üí 2 ‚Üí 3 ‚Üí 4 ‚Üí ... ‚Üí 10,000\n",
        "\n",
        "With snapshot at event 8,000\n",
        "Load snapshot (state at event 8,000)\n",
        "Replay events:\n",
        "8,001 ‚Üí 8,002 ‚Üí ...\n",
        "\n",
        "\n",
        "üöÄ Much faster.\n",
        "\n",
        "When snapshots are NOT needed\n",
        "\n",
        "Don‚Äôt use snapshots if:\n",
        "\n",
        "Jobs are short\n",
        "\n",
        "Few events (< 50)\n",
        "\n",
        "Replay is cheap\n",
        "\n",
        "Example:\n",
        "\n",
        "Simple CRUD apps\n",
        "\n",
        "Short synchronous requests\n",
        "\n",
        "Interview-ready answer (memorize this)\n",
        "\n",
        "‚ÄúSnapshots are saved at meaningful checkpoints ‚Äî such as phase completion, high event volume, or before risky operations ‚Äî to reduce replay cost. The system still treats the append-only event log as the source of truth, and snapshots are only an optimization.‚Äù\n",
        "\n",
        "That sentence alone = üíØ in interviews.\n",
        "\n",
        "TL;DR (one screen summary)\n",
        "Question\tAnswer\n",
        "When are snapshots saved?\tOnly at chosen checkpoints\n",
        "Are they automatic?\t‚ùå No\n",
        "Why save them?\tFaster recovery\n",
        "Source of truth?\tAppend-only events\n",
        "Can system work without snapshots?\t‚úÖ Yes\n",
        "Do snapshots replace events?\t‚ùå Never"
      ],
      "metadata": {
        "id": "gRSirxP_l4kM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic\tBad Snapshot\tGood Snapshot\n",
        "Truth source\tSnapshot overwrites truth\tEvents are truth\n",
        "Writes\tsnapshot every event\tsnapshot by policy\n",
        "Replay\timpossible\talways possible\n",
        "Audit\tnone\tfull history\n",
        "Recovery\tfragile\treliable\n",
        "Storage\thuge blobs\tminimal checkpoints\n",
        "Safety\tcan double-apply work\tsafe replay boundary (last_event_id)"
      ],
      "metadata": {
        "id": "6S97IenKmZeF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SVWx9gfvk2ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mJDlLMZwhTTI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}